# -*- coding: utf-8 -*-
"""Scrapper_RAG_With_FASTAPI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TzzzO6q_0SgKuIe4fJEWNr40yj_a-OAN
"""

!pip install fastapi uvicorn bs4 requests transformers pyngrok python-multipart chromadb pydantic PyPDF2 torch sentence-transformers accelerate jinja2 nest-asyncio

import os
import torch
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import requests
from bs4 import BeautifulSoup
from pyngrok import ngrok
import PyPDF2
import chromadb
from transformers import AutoModelForCausalLM, AutoTokenizer
from sentence_transformers import SentenceTransformer
import uvicorn
import nest_asyncio
from fastapi.templating import Jinja2Templates
from fastapi.responses import HTMLResponse
from fastapi import Request
from io import BytesIO

# Set your ngrok authentication token
ngrok.set_auth_token("2m3INDfD7mYEcHw8VB1STZuofFc_UBjx4mHCjjpu5iZbbxgN")

# Initialize Qwen model for chat
qwen_tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-3B-Instruct")
qwen_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-3B-Instruct",
    device_map="auto",
    torch_dtype=torch.float16
)

# Initialize Sentence Transformer for embeddings
embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

"""-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"""

app = FastAPI(title="Content Processing and Chat Service")


# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize Chroma client for vector storage
chroma_client = chromadb.PersistentClient(path="./chroma_storage")
collection = chroma_client.get_or_create_collection(name="content_collection")

# Add templates directory
templates = Jinja2Templates(directory="templates")



class URLProcessRequest(BaseModel):
    url: str

class ChatRequest(BaseModel):
    query: str
    source_id: str

def get_embedding(text):
    """Generate embeddings using Sentence Transformers"""
    return embedding_model.encode(text).tolist()

def chunk_text(text, max_tokens=500):
    """Chunk text into manageable pieces"""
    tokens = qwen_tokenizer.encode(text)
    chunks = []

    for i in range(0, len(tokens), max_tokens):
        chunk = tokens[i:i + max_tokens]
        chunks.append(qwen_tokenizer.decode(chunk))

    return chunks

@app.get("/", response_class=HTMLResponse)
async def serve_ui(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

@app.post("/process-url")
async def process_url(request: URLProcessRequest):
    try:
        # Fetch webpage content
        response = requests.get(request.url)
        soup = BeautifulSoup(response.text, 'html.parser')

        # Extract main content (you might want to improve this)
        text = soup.get_text()

        # Generate a unique ID for this source
        source_id = f"url_{hash(request.url)}"

        # Chunk and embed the text
        text_chunks = chunk_text(text)

        # Store chunks in vector database
        for i, chunk in enumerate(text_chunks):
            embedding = get_embedding(chunk)
            collection.add(
                embeddings=[embedding],
                documents=[chunk],
                ids=[f"{source_id}_chunk_{i}"]
            )

        return {"source_id": source_id, "status": "Processed successfully"}

    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@app.post("/process-pdf")
async def process_pdf(file: UploadFile = File(...)):
    try:
        # Read the uploaded file as bytes
        pdf_content = await file.read()

        # Wrap the bytes content in a BytesIO object
        pdf_file = BytesIO(pdf_content)

        # Use PyPDF2 to read the PDF
        pdf_reader = PyPDF2.PdfReader(pdf_file)

        # Extract text from all pages
        full_text = ""
        for page in pdf_reader.pages:
            full_text += page.extract_text()

        # Generate a unique ID for this source
        source_id = f"pdf_{hash(file.filename)}"

        # Chunk and embed the text
        text_chunks = chunk_text(full_text)

        # Store chunks in the vector database
        for i, chunk in enumerate(text_chunks):
            embedding = get_embedding(chunk)
            collection.add(
                embeddings=[embedding],
                documents=[chunk],
                ids=[f"{source_id}_chunk_{i}"]
            )

        return {"source_id": source_id, "status": "Processed successfully"}

    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@app.post("/chat")
async def chat_with_content(request: ChatRequest):
    try:
        # Get query embedding
        query_embedding = get_embedding(request.query)

        # Perform similarity search
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=2 # Top 2 most relevant chunks

        )

         # Check if results are empty
        if not results or not results.get('documents'):
            raise HTTPException(status_code=404, detail="No relevant context found for the given source ID.")


        # Combine relevant chunks for context
        context = " ".join(results['documents'][0])

        # Prepare the prompt for Qwen model
        prompt = f"""Context: {context}

Question: {request.query}

Please provide a helpful and concise answer based on the given context.\n\n
FINAL ANSWER:-->>
 """

        # Tokenize the prompt
        inputs = qwen_tokenizer(prompt, return_tensors="pt").to(qwen_model.device)

        # Generate response
        outputs = qwen_model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            top_p=0.9
        )

        # Decode the response
        response = qwen_tokenizer.decode(outputs[0], skip_special_tokens=True)

        return {
            "response": response,
            "context_used": context
        }

    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))